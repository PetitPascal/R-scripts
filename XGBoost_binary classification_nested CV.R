#----------------------------------------------------------------
#### Configurations ####

## Disabling memory torture
gctorture(FALSE)

## Installing packages if needed
pack_needed<-c("data.table","tidyverse","mllrnrs","broom","doParallel","foreach",
               "splitTools","conflicted","grid","gridExtra","RColorBrewer","mlbench",
               "mlexperiments","caret","MLmetrics","patchwork",
               "xgboost","parallel")
for (i in 1:length(pack_needed)){
  if(pack_needed[i]%in%.packages(all.available=TRUE)){
  }else{
    install.packages(pack_needed[i])
  }
}

## Package loading
library(tidyverse)
library(data.table)
library(broom)
library(doParallel)
library(foreach)
library(pROC)
library(grid)
library(gridExtra)
library(conflicted)
library(RColorBrewer)
library(mlexperiments)
library(mllrnrs)
library(mlbench)
library(caret)
library(MLmetrics)
library(xgboost)
library(patchwork)

## Preventing package conflicts
conflict_prefer("select", "dplyr") 
conflict_prefer("filter", "dplyr") 
conflict_prefer("slice", "dplyr")
conflict_prefer("alpha", "scales")
conflict_prefer("auc", "pROC")
conflict_prefer("roc", "pROC")

## Setting the working directory
setwd("E:/Pascal/Articles/Collab/TED")

## Setting seed
seed <- 123

## Setting cores
if (isTRUE(as.logical(Sys.getenv("_R_CHECK_LIMIT_CORES_")))) {
  ncores <- 2L
} else {
  ncores <- ifelse(
    test = parallel::detectCores() > 4,
    yes = 4L,
    no = ifelse(
      test = parallel::detectCores() < 2L,
      yes = 1L,
      no = parallel::detectCores()
    )
  )
}

## Setting mlexperiments package options
options("mlexperiments.bayesian.max_init" = 10L)
options("mlexperiments.optim.xgb.nrounds" = 100L)
options("mlexperiments.optim.xgb.early_stopping_rounds" = 10L)

#----------------------------------------------------------------
#### Creating functions ####

## Function for plotting SHAP values
plot.shap.summary <- function(data_long){
  x_bound <- max(abs(data_long$value))
  require('ggforce')
  plot1 <- ggplot(data = data_long)+
    coord_flip() + 
    geom_sina(aes(x = variable, y = value, color = stdfvalue)) +
    geom_text(data = unique(data_long[, c("variable", "mean_value"), with = F]),
              aes(x = variable, y=-Inf, label = sprintf("%.3f", mean_value)),
              size = 3, alpha = 0.7,
              hjust = -0.2, 
              fontface = "bold") + # bold
    scale_color_gradient(low="#FFCC33", high="#6600CC", 
                         breaks=c(0,1), labels=c("Low","High")) +
    theme_bw() + 
    theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(),
          legend.position="bottom") + 
    geom_hline(yintercept = 0) +
    scale_y_continuous(limits = c(-x_bound, x_bound)) +
    scale_x_discrete(limits = rev(levels(data_long$variable)) 
    ) + 
    labs(y = "SHAP value (impact on model output)", x = "", color = "Feature value") 
  return(plot1)
}

## Function for standardizing feature values into the same range
std1 <- function(x){
  return ((x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T)))
}

## Function for formatting the display of summary statistics
test_format<-function(x){
  x<-as.numeric(x)
  
  if(is.na(x)|x==""|x=="NA"){
    x<-""
    signe<-"pos"
  }else{
    signe<-if_else(x<0,"neg","pos")
    x<-abs(x)
    
    if(x<0.01|x>=10000){
      x<-format(signif(x,3),scientific = T)
      
      if(nchar(x)==8&substr(x,4,4)!="e"){
        if(as.numeric(substr(x,4,4))>=5){
          
          x1<-as.numeric(substr(x,1,4))+0.1
          x2<-substr(x,5,8)
          x<-paste(substr(x1,1,3),x2,sep="")
        }else{
          x<-paste(substr(x,1,3),substr(x,5,8),sep="")
        }
      }else{
        x<-x
      }
      
    }else{
      x<-signif(x,3)
      if(nchar(x)==6){
        x<-as.numeric(substr(x,1,5))
      }
      
      if(nchar(x)==5){
        if(as.numeric(substr(x,5,5))>=5){
          x<-x+0.01
          x<-substr(x,1,4)
        }else{
          x<-substr(x,1,4)
        }
      }else{
        x<-as.character(x)
      }
      
      if(nchar(x)==3&str_detect(x,"[.]")==T){
        x<-paste(x,"0",sep="")
      }
      
    }
    
  }
  
  if(signe=="neg"){
    x<-paste("-",x,sep="")
  }
  
  return(x)
  
}

# Function for determining the direction (i.e.: how a feature impacts the model)
determine_direction<-function(df,feature_col,shap_col){
  
  # extracting vectors
  x<-df[[feature_col]]
  s<-df[[shap_col]]
  
  # removing NA
  valid<-complete.cases(x,s)
  x<-x[valid]
  s<-s[valid]
  
  # for cases when all values are identical
  if(length(unique(x))<=1||length(unique(s))<=1){
    return("undefined")
  }
  
  # for cases when the feature is binary or sparse
  if(length(unique(x))<=2||quantile(x,0.75)==0){
    mean_diff<-mean(s[x>0],na.rm=T)-mean(s[x<=0],na.rm=T)
    
    return(ifelse(mean_diff>0,'promoting factor',ifelse(mean_diff<0,"limiting factor","neutral")))
  }
  
  # for cases when the feature is continuous
  tryCatch({
    gam_model<-gam(s~s(x), method='REML')
    derivs<-derivatives(gam_model, term='s(x)')
    mean_deriv<-mean(derivs$derivative,na.rm=T)
    
    return(ifelse(mean_deriv>0,'promoting factor',ifelse(mean_deriv<0,"limiting factor","neutral")))
  }, error=function(e){
    
    # if GAM fails, Spearman correlation is computed
    rho<-suppressWarnings(cor(x,s,method="spearman"))
    if(is.na(rho)) return("undefined")
    if(abs(rho)<0.05) return("neutral")
    return(ifelse(rho>0,"promoting factor","limiting factor"))
  }
  )
}

## not including function (opposite function of %in%)
`%ni%`<-Negate('%in%')

#----------------------------------------------------------------
#### Data import and preprocessing ####

## Importing the variable thesaurus
nomenclature<-as_tibble(read.csv("E:/Pascal/Articles/Collab/TED/nomenclature.csv", sep=";"))
nomenclature2<-nomenclature

## Importing the clean dataset
clean_data<-as_tibble(read.csv2("E:/Pascal/Articles/Collab/TED/TED_01-09-25.csv"))
clean_data<-clean_data %>% select(-c(ID)) # removing participant ID
clean_data_save<-clean_data

## Reorganising the dataset to ensure TM is the first column
clean_data<-clean_data %>% select(outcome,colnames(clean_data)[which(colnames(clean_data) %ni% c("outcome"))])

#--------------------------
## Transforming the clean dataset into data.table
dataset <- clean_data |> data.table::as.data.table()

## Creating vectors with the independent and dependent variable names
feature_cols <- colnames(dataset)[2:ncol(dataset)]
target_col <- "outcome"

## Identifying unordered factors
categorical_var<-nomenclature %>% filter(Variable %in% colnames(clean_data)) %>% 
                 filter(Var_type %in% c("categorical")) %>% select(Variable) %>% pull

## One-hot encoding of unordered factors
if (length(categorical_var) > 0) {
  for (col in categorical_var) {
    dataset[[col]] <- addNA(dataset[[col]])
  }
  dummies <- model.matrix(~ -1 + ., data = dataset[, ..categorical_var])
  dataset <- cbind(dataset[, !..categorical_var], dummies)
}

## Creating the stratified 70/30 train-test split
set.seed(seed)
data_split <- splitTools::partition(
  y = dataset[[target_col]],
  p = c(train = 0.7, test = 0.3),
  type = "stratified",
  seed = seed
)

## Preparing training and test datasets

# train dataset
X_train <- as.matrix(dataset[data_split$train, setdiff(colnames(dataset), target_col), with = FALSE])
y_train <- as.integer(dataset[data_split$train, get(target_col)])

# test dataset
X_test  <- as.matrix(dataset[data_split$test, setdiff(colnames(dataset), target_col), with = FALSE])
y_test  <- as.integer(dataset[data_split$test, get(target_col)])

#----------------------------------------------------------------
#### 10-fold nested cross-validation (CV) ####

#--------------------------
## Performing the outer CV on training set

# Creating the outer folds
outer_folds <- splitTools::create_folds(y_train, k = 10, type = "stratified", seed = seed)

# Creating empty lists to store results
outer_results <- list()
best_params_all <- list()

for (outer_idx in seq_along(outer_folds)) { # for each outer fold
  
  # selecting an outer fold
  val_idx <- outer_folds[[outer_idx]]
  train_idx_cv <- setdiff(seq_len(nrow(X_train)), val_idx)
  
  # creating training and test data sets for a given outer fold
  X_tr <- X_train[train_idx_cv, , drop = FALSE]
  y_tr <- y_train[train_idx_cv]
  X_val <- X_train[val_idx, , drop = FALSE]
  y_val <- y_train[val_idx]
  
  # Handling imbalance data
  Weight <- max(table(y_tr)) / min(table(y_tr))
  
  #--------------------------
  ## Performing the inner CV for tuning
  
  # Creating the inner folds
  inner_folds <- splitTools::create_folds(y_tr, k = 10, type = "stratified", seed = seed)
  
  # Creating random parameter space for hyperparameter tuning
  param_list <- expand.grid(
    subsample = seq(0.5, 1, 0.25),
    colsample_bytree = seq(0.5, 1, 0.25),
    min_child_weight = c(1, 5, 10),
    learning_rate = c(0.05, 0.1, 0.3),
    max_depth = c(3, 5, 7),
    scale_pos_weight = Weight
  ) %>% dplyr::slice_sample(n = 30, replace = TRUE) # Limiting space to 30 combinations for computational efficiency and environmental sustainability considerations
  
  # Initializing best parameters
  best_auc <- -Inf
  best_params <- NULL
  
  for (i in 1:nrow(param_list)) {
    
    # Initializing cross validation
    xgb_cv <- mlexperiments::MLCrossValidation$new(
      learner = mllrnrs::LearnerXgboost$new(metric_optimization_higher_better = TRUE),
      fold_list = inner_folds,
      ncores = 2,
      seed = 0
    )
    
    # Defining the learner arguments
    xgb_cv$learner_args <- c(
      as.list(param_list[i, ]),
      list(objective = "binary:logistic", eval_metric = "logloss", nrounds = 100L)
    )
    xgb_cv$performance_metric_args <- list(positive = "1")
    xgb_cv$performance_metric <- mlexperiments::metric("auc")
    xgb_cv$set_data(x = X_tr, y = y_tr)
    
    # Executing CV tuning
    res_cv <- xgb_cv$execute()
    
    # Calculating the mean AUROC
    mean_auc <- mean(sapply(res_cv$results$folds, function(f) f[[5]]$performance))
    
    if (!is.na(mean_auc) && mean_auc > best_auc) {
      best_auc <- mean_auc
      best_params <- param_list[i, ]
    }
  }
  
  # If no params are selected, fallback to default
  if (is.null(best_params)) {
    best_params <- data.frame(
      subsample = 1,
      colsample_bytree = 1,
      min_child_weight = 1,
      learning_rate = 0.1,
      max_depth = 3,
      scale_pos_weight = Weight
    )
  }
  
  best_params_all[[outer_idx]] <- best_params
  
  ## Evaluating on outer fold
  
  # Building the model
  dtrain <- xgboost::xgb.DMatrix(data = X_tr, label = y_tr)
  dval   <- xgboost::xgb.DMatrix(data = X_val, label = y_val)
  
  xgb_outer <- xgboost::xgb.train(
    params = as.list(best_params),
    data = dtrain,
    nrounds = 100,
    objective = "binary:logistic",
    eval_metric = "auc",
    verbose = 0
  )
  
  # Assessing the performance
  val_pred <- predict(xgb_outer, newdata = dval)
  roc_curve <- pROC::roc(y_val, val_pred)
  auc <- signif(pROC::auc(roc_curve), 3)
  F1 <- MLmetrics::F1_Score(ifelse(val_pred > 0.5, 1, 0), y_val)
  
  ## Saving CV results
  outer_results[[outer_idx]] <- data.frame(Fold = outer_idx, AUROC = auc, F1 = F1)
}

# Combining results from outer CV into a summary table
outer_summary <- dplyr::bind_rows(outer_results)
outer_summary

#--------------------------
## Retraining final model on full training set

# Picking the best params (e.g., from highest outer AUROC)
best_idx <- which.max(outer_summary$AUROC)
final_params <- best_params_all[[best_idx]]

# Retraining final model
dtrain_full <- xgboost::xgb.DMatrix(data = X_train, label = y_train)
xgb_final <- xgboost::xgb.train(
  params = as.list(final_params),
  data = dtrain_full,
  nrounds = 100,
  objective = "binary:logistic",
  eval_metric = "auc",
  verbose = 1
)

#--------------------------
## Final evaluation on holdout test dataset

# Predicting outcome in holdout test dataset
test_pred <- predict(xgb_final, newdata = X_test)
val_pred2<-ifelse(test_pred > 0.5, 1, 0)
val_pred<-test_pred
Val_obs<-y_test

# AUROC
roc_curve <- roc(Val_obs, val_pred) #AUROC curve
AUC_CI<-signif(ci.auc(roc_curve),3) #AUROC 95% CI
auc<-unique(median(AUC_CI)) #AUROC
AUC_CI<-paste(unique(median(AUC_CI))," (",unique(min(AUC_CI)),"; ",unique(max(AUC_CI)),")",sep="")

# Model accuracy
tmp1<-as_tibble(data.frame(Param=names(caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$overall),
                           Val=caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$overall))

tmp1<-tmp1 %>% mutate(Val=signif(Val,3)) %>% 
  pivot_wider(names_from=Param,values_from=Val) %>%
  mutate(Accuracy=paste(Accuracy," (",AccuracyLower,"; ",AccuracyUpper,")",sep="")) %>%
  select(Accuracy) %>% pull

# Other model characteristics
tmp2<-as_tibble(data.frame(Param=names(caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$byClass),
                           Val=caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$byClass)) %>%
  filter(Param %in% c("Specificity","Prevalence","Neg Pred Value")) %>%
  pivot_wider(names_from=Param,values_from = Val)

colnames(tmp2)[which(colnames(tmp2)=="Neg Pred Value")]<-"Neg_Pred_Value"

tmp3<-as_tibble(data.frame(Param=names(caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$overall),
                           Val=caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")$overall)) %>%
  pivot_wider(names_from=Param,values_from = Val)

# Adding population information
n_mal<-clean_data %>% filter(outcome==1) %>% nrow # number of TM users
n_sain<-clean_data %>% filter(outcome==0) %>% nrow # number of non-TM users
per_mal<-signif(n_mal/(n_sain+n_mal)*100,3) # % of non-TM users
per_sain<-signif(n_sain/(n_sain+n_mal)*100,3) # % of non-TM users
n_mal<-paste(n_mal," (",per_mal,"%)",sep="")
n_sain<-paste(n_sain," (",per_sain,"%)",sep="")

# Creating a confusion matrix with everything
confus_mat_temp<-as_tibble(data.frame(clean_data_mal=n_mal,
                                      clean_data_sain=n_sain,
                                      AUC=AUC_CI,
                                      Accuracy=tmp1,
                                      F1_score=F1_Score(val_pred2,Val_obs),
                                      GainAUC=GainAUC(val_pred,Val_obs),
                                      PRAUC=PRAUC(val_pred,Val_obs),
                                      PPV=Precision(val_pred2,Val_obs),
                                      Precision=Precision(val_pred2,Val_obs),
                                      Recall=MLmetrics::Recall(val_pred2,Val_obs),
                                      Sensitivity=Sensitivity(val_pred2,Val_obs)))

confus_mat_temp<-bind_cols(confus_mat_temp,tmp2) %>% mutate(balanced_accuracy=(Sensitivity+Specificity)/2)
confus_mat_temp<-bind_cols(confus_mat_temp,tmp3)

# Creating the confusion matrix from the caret package
confus_mat_caret<-broom::tidy(caret::confusionMatrix(factor(val_pred2), factor(Val_obs),mode = "everything",positive="1")) %>% select(-class)

#--------------------------
## Saving CV summary table, confusion matrix and best hyperparameters
write.table(outer_summary,paste("CV summary table_",Sys.Date(),".csv"),sep=";",col.names=T,row.names=F)
write.table(confus_mat_temp,paste("Confusion matrix_",Sys.Date(),".csv"),sep=";",col.names=T,row.names=F)
write.table(confus_mat_caret,paste("caret confusion matrix_",Sys.Date(),".csv"),sep=";",col.names=T,row.names=F)
write.table(as_tibble(final_params),paste("Best hyperparameters_",Sys.Date(),".csv"),sep=";",col.names=T,row.names=F)

#----------------------------------------------------------------
#### SHAP analysis ####

#- - - - - - - - - -
## Calculating SHAP values
contr <- predict(xgb_final, as.matrix(X_test[,xgb_final$feature_names]), predcontrib = TRUE)
shap<-as_tibble(contr)
shap_contrib <- as.data.table(contr)

#- - - - - - - - - -
## Removing BIAS term
shap_contrib <- shap_contrib[, !grepl("bias", names(shap_contrib), ignore.case = TRUE), with=FALSE]

#- - - - - - - - - -
## Computing mean SHAP score
mean_shap_score <- colMeans(abs(shap_contrib))[order(colMeans(abs(shap_contrib)), decreasing = T)]

#- - - - - - - - - -
## Reshaping SHAP values to long format
shap_score<-shap_contrib
shap_score_sub <- as.data.table(shap_score)
shap_score_sub <- shap_score_sub[, names(mean_shap_score), with = F]
shap_score_long <- melt.data.table(shap_score_sub, measure.vars = colnames(shap_score_sub))

#- - - - - - - - - -
## Matching feature values
fv_sub <- as.data.table(X_test)[, names(mean_shap_score), with = F]
fv_sub_long <- melt.data.table(fv_sub, measure.vars = colnames(fv_sub))
fv_sub_long[, stdfvalue := std1(value), by = "variable"]
names(fv_sub_long) <- c("variable", "rfvalue", "stdfvalue" )

#- - - - - - - - - -
## Merging  SHAP and feature values
shap_long2 <- cbind(shap_score_long, fv_sub_long[,c('rfvalue','stdfvalue')])
shap_long2[, mean_value := mean(abs(value)), by = variable]
setkey(shap_long2, variable)

#- - - - - - - - - -
## Computting summary statistics per feature
Shap_val<-as_tibble(shap_long2) %>% 
          group_by(variable) %>% 
          mutate(IC_2.5=quantile(abs(value),0.025),
                 IC_97.5=quantile(abs(value),0.975),
                 mean_val=mean(abs(value),na.rm=T)) %>%
          select(variable,mean_val,IC_2.5,IC_97.5) %>% 
          distinct %>% 
          arrange(desc(mean_val)) %>%
          mutate(mean_SHAP=paste(signif(mean_val,3)," (",signif(IC_2.5,3),"; ",signif(IC_97.5,3),")",sep="")) %>% ungroup

colnames(Shap_val)[1]<-"feature"

#- - - - - - - - - -
## Formmating SHAP values
Shap_val<-Shap_val %>% rowwise() %>% mutate(mean_val=test_format(mean_val),
                                            IC_2.5=test_format(IC_2.5),
                                            IC_97.5=test_format(IC_97.5)) %>%
          ungroup %>% mutate(mean_SHAP=paste(mean_val," (",IC_2.5,"; ",IC_97.5,")",sep="")) %>% 
          mutate(mean_SHAP=if_else(mean_SHAP=="0 (0; 0)"|mean_SHAP=="0e+00 (0e+00; 0e+00)","0",mean_SHAP))

#- - - - - - - - - -
## Pivoting SHAP and features long format
shap_tempo <- shap %>% 
  rowid_to_column("id") %>%
  pivot_longer(-id, names_to = "feature", values_to = "shap_value")

tempopo <- X_test[, xgb_final$feature_names] %>%
  as_tibble() %>%
  rowid_to_column("id") %>%
  pivot_longer(-id, names_to = "feature", values_to = "feature_value")

tempopo <- left_join(tempopo, shap_tempo, by = c("id", "feature"))

#- - - - - - - - - -
## SHAP summary and merging labels
test_save<-xgb.ggplot.shap.summary(as.matrix(X_test[,xgb_final$feature_names]), contr, model = xgb_final,top_n=25)
test_save<-as_tibble(test_save$data)

colnames(nomenclature2)[1]<-"feature"

tempopo<-left_join(tempopo,nomenclature2,by=base::intersect(colnames(tempopo),colnames(nomenclature2)))
tempopo<-left_join(tempopo,Shap_val,by=base::intersect(colnames(tempopo),colnames(Shap_val)))
Shap_val<-left_join(Shap_val,nomenclature2,by="feature")

test3<-Shap_val %>% arrange(desc(as.numeric(mean_val)))

#- - - - - - - - - -
## SHAP directionality
direction_impact<-tempopo %>% group_by(feature) %>%
  group_map(~tibble(feature=.y$feature,
                    direction=determine_direction(.x,"feature_value","shap_value")
  )) %>%
  bind_rows

test<-left_join(tempopo,direction_impact,by="feature")

test<-left_join(test,nomenclature2,by=base::intersect(colnames(test),colnames(nomenclature2)))
test<-left_join(test,Shap_val,by=base::intersect(colnames(test),colnames(Shap_val)))

test_tmp<-test %>% mutate(impact=direction) %>%
  select(Def, mean_val,IC_2.5,IC_97.5,impact,mean_SHAP) %>% 
  distinct() %>%
  mutate(mean_val=as.numeric(mean_val),
         IC_2.5=as.numeric(IC_2.5),
         IC_97.5=as.numeric(IC_97.5)) %>%
  mutate(mean_val=if_else(impact=="limiting factor",-mean_val,mean_val),
         IC_2.5=if_else(impact=="limiting factor",-IC_2.5,IC_2.5),
         IC_97.5=if_else(impact=="limiting factor",-IC_97.5,IC_97.5))

test_tmp<-test_tmp %>% arrange(desc(mean_val)) %>% filter(mean_val!=0) %>% filter(!is.na(mean_val))

#- - - - - - - - - -
## Plotting SHAP

# reordoring factors
ordre_def1<-test_tmp %>% filter(mean_val>=0) %>% arrange(desc(IC_2.5))
ordre_def2<-test_tmp %>% filter(mean_val<0) %>% arrange(desc(IC_97.5))
ordre_def<-bind_rows(ordre_def1,ordre_def2)

test_tmp$Def<-factor(test_tmp$Def,levels=rev(unique(ordre_def$Def)))
test_tmp$impact<-factor(test_tmp$impact,levels=c("limiting factor","neutral","promoting factor"),
                        labels=c("limiting factor","no impact","promoting factor"))

# Plot 1: error bar
plot1<-ggplot(test_tmp,aes(y=Def,x=mean_val,col=impact))+
  geom_point()+
  geom_errorbar(aes(xmin = IC_2.5, xmax = IC_97.5))+
  scale_x_continuous("mean |SHAP value|",label=function(x) abs(x))+
  geom_text(aes(x=IC_97.5,y=Def,label=abs(IC_97.5)),size=5,position=position_nudge(x = if_else(test_tmp$IC_97.5>=0,0.15,-0.15)))+
  scale_y_discrete("")+
  theme_bw()+
  scale_color_manual("Direction:",
                     na.value="white",
                     values=rev(c("white","#C35C33","#40B696")))+
  theme(strip.text.x = element_text(size = 16, colour = "black", angle = 0),
        strip.background = element_rect(fill="#A6DDCE", colour="black", size=1),
        axis.text.y = element_text(size=16,color="black"),
        axis.text.x = element_text(size=16,color="black"),
        axis.title=element_text(size=16,face="bold",color="black"),
        legend.text = element_text(size = 16, face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.position="bottom",
        axis.line = element_line(color = "black",size = 0.1, linetype = "solid"))

# Exporting plot 1
ggsave(plot1,
       file=paste("SHAP_errorbar_",
                  Sys.Date(),".pdf",sep=""),
       dpi=600,width=60,height=30,units = "cm",limitsize=F)

# Plot 2: barplot
ordre_def<-test_tmp %>% arrange(desc(mean_val))
test_tmp$Def<-factor(test_tmp$Def,levels=rev(unique(ordre_def$Def)))

plot2<-ggplot(test_tmp,aes(y=Def,x=mean_val,fill=impact))+
  geom_bar(stat = "identity",col='black')+
  geom_text(aes(x=mean_val,y=Def,label=mean_SHAP),size=6,position=position_nudge(x = if_else(test_tmp$mean_val>=0,0.2,-0.2)))+
  scale_x_continuous("mean |SHAP value|",label=function(x) abs(x),limits=c(-0.75,1.25))+
  scale_y_discrete("")+
  theme_bw()+
  scale_fill_manual("Direction:",
                    na.value="white",
                    values=c("#A6DDCE","#F9CBC2"))+
  theme(strip.text.x = element_text(size = 16, colour = "black", angle = 0),
        strip.background = element_rect(fill="#A6DDCE", colour="black", size=1),
        axis.text = element_text(size=16,color="black", face = "bold"),
        axis.title=element_text(size=16,face="bold",color="black"),
        legend.text = element_text(size = 16, face = "bold"),
        legend.title = element_text(size = 16, face = "bold"),
        legend.position="bottom",
        axis.line = element_line(color = "black",size = 0.1, linetype = "solid"))

# Exporting plot 2
ggsave(plot2,
       file=paste("SHAP_barplot_",
                  Sys.Date(),".pdf",sep=""),
       dpi=600,width=60,height=30,units = "cm",limitsize=F)

# Plot 3: dispersion

bornes<-test %>% select(feature_value) %>% summarize(min=floor(min(feature_value,na.rm=T)),
                                                     max=ceiling(max(feature_value,na.rm=T)))
bornes_shap<-test %>% select(shap_value) %>% summarize(min=min(shap_value,na.rm=T),max=max(shap_value,na.rm=T))

test<-test %>% group_by(feature) %>% 
  mutate(feature_value2=(feature_value-min(feature_value,na.rm=T))/(max(feature_value,na.rm=T)-min(feature_value,na.rm=T))) %>%
  ungroup

test3<-test %>% filter(mean_SHAP!="0")
ordre_Def<-test3 %>% select(Def,mean_val) %>% distinct %>% mutate(mean_val=as.numeric(mean_val)) %>% arrange(mean_val)
test3$Def<-factor(test3$Def,level=ordre_Def$Def)
test2<-test3 %>% select(Def,mean_SHAP) %>% distinct

plot3<-ggplot2::ggplot(test3, ggplot2::aes(x = Def, y = shap_value,colour = feature_value2)) + 
  ggplot2::geom_jitter(width = 0.1) + 
  scale_color_gradientn("Feature value",labels=c("Low", "High"),
                        breaks=c(0,1),
                        limits=c(0,1),
                        colours=c("blue",scales::alpha("blue",0.75),
                                  scales::alpha("blue",0.5),
                                  scales::alpha("blue",0.35),
                                  scales::alpha("#FFA5001A",0.25),
                                  scales::alpha("red",0.35),
                                  scales::alpha("red",0.5),
                                  scales::alpha("red",0.75),
                                  "red"),
                        na.value ="lightgrey",
                        guide = guide_colorbar(barheight = 50,
                                               title.theme = element_text(angle = 90, hjust = 0.5, vjust = 0,size = 16, face = "bold"),
                                               title.position = "left"
                        ))+
  ggplot2::geom_abline(slope = 0,intercept = 0, colour = "darkgrey") + 
  scale_x_discrete("Feature")+
  scale_y_continuous("SHAP value (impact on model output)",limits=c(bornes_shap$min,bornes_shap$max*1.5))+
  ggplot2::coord_flip()+
  geom_text(data=test2,aes(x = Def, y = bornes_shap$max*1.35,label=mean_SHAP),size=5,col="black")+
  theme_bw()+
  theme(strip.text.x = element_text(size = 16, colour = "black", angle = 0),
        strip.background = element_rect(fill=scales::alpha('#009E73', 0.2), colour="black", size=1),
        axis.text = element_text(size=16,color="black"),
        axis.title=element_text(size=16,face="bold",color="black"),
        legend.text = element_text(size = 16, face = "bold"),
        axis.line = element_line(color = "black",size = 0.1, linetype = "solid"))

# Exporting plot 3
ggsave(plot3,
       file=paste("SHAP_dispersion_",
                  Sys.Date(),".pdf",sep=""),
       dpi=600,width=60,height=30,units = "cm",limitsize=F)

#----------------------------------------------------------------
#### Saving SHAP results

# SHAP values
fwrite(Shap_val,paste("Shap_val_",Sys.Date(),".csv",sep=""), sep = ";", row.names=FALSE)
fwrite(test_tmp,paste("test_tmp_",Sys.Date(),".csv",sep=""), sep = ";", row.names=FALSE)
fwrite(test,paste("test_",Sys.Date(),".txt",sep=""), sep = ";", row.names=FALSE)
fwrite(shap,paste("shap_",Sys.Date(),".txt",sep=""), sep = ";", row.names=FALSE)
fwrite(shap_score_sub,paste("shap_score_sub_",Sys.Date(),".txt",sep=""), sep = ";", row.names=FALSE)
